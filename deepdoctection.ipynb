{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bfa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorpack\n",
    "!pip install python-doctr==0.9.0\n",
    "!pip install deepdoctection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdoctection as dd\n",
    "from IPython.core.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Instantiate the built-in analyzer\n",
    "analyzer = dd.get_dd_analyzer()\n",
    "\n",
    "# Analyze your bank statement PDF\n",
    "df = analyzer.analyze(path=\"/content/Revoult Bank statement.pdf\")\n",
    "df.reset_state()  # Initialize the pipeline\n",
    "\n",
    "# Process the document\n",
    "doc = iter(df)\n",
    "page = next(doc)\n",
    "\n",
    "# Visualize the detected layout and tables\n",
    "image = page.viz(show_figures=True, show_residual_layouts=True)\n",
    "plt.figure(figsize=(25, 17))\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "\n",
    "# Extract table data as HTML\n",
    "if page.tables:\n",
    "    HTML(page.tables[0].html)\n",
    "\n",
    "# Extract all text content\n",
    "print(page.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0832bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all tables in the document\n",
    "for i, table in enumerate(page.tables):\n",
    "    print(f\"Table {i+1}:\")\n",
    "    print(HTML(table.html))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49addcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdoctection as dd\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_tables_deepdoctection(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract tables from bank statement using deepdoctection\n",
    "    \"\"\"\n",
    "    # Initialize analyzer\n",
    "    analyzer = dd.get_dd_analyzer()\n",
    "\n",
    "    # Analyze document\n",
    "    df = analyzer.analyze(path=pdf_path)\n",
    "    df.reset_state()\n",
    "\n",
    "    all_tables = []\n",
    "\n",
    "    # Process each page\n",
    "    page_num = 1\n",
    "    for page in df:\n",
    "        print(f\"\\n=== PAGE {page_num} ===\")\n",
    "\n",
    "        # Check if tables exist\n",
    "        if page.tables:\n",
    "            print(f\"Found {len(page.tables)} table(s) on page {page_num}\")\n",
    "\n",
    "            for i, table in enumerate(page.tables):\n",
    "                print(f\"\\nTable {i+1} on Page {page_num}:\")\n",
    "\n",
    "                # Method 1: Get HTML format\n",
    "                try:\n",
    "                    html_table = table.html\n",
    "                    print(\"HTML Table:\")\n",
    "                    display(HTML(html_table))\n",
    "                except:\n",
    "                    print(\"HTML extraction failed\")\n",
    "\n",
    "                # Method 2: Extract structured data\n",
    "                try:\n",
    "                    table_data = []\n",
    "                    for row in table.rows:\n",
    "                        row_data = []\n",
    "                        for cell in row.cells:\n",
    "                            row_data.append(cell.text.strip())\n",
    "                        table_data.append(row_data)\n",
    "\n",
    "                    if table_data and len(table_data) > 1:\n",
    "                        # Create DataFrame\n",
    "                        df_table = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "                        print(\"\\nStructured Table:\")\n",
    "                        print(df_table)\n",
    "\n",
    "                        # Save to CSV\n",
    "                        csv_filename = f\"bank_table_page{page_num}_table{i+1}.csv\"\n",
    "                        df_table.to_csv(csv_filename, index=False)\n",
    "                        print(f\"Saved to: {csv_filename}\")\n",
    "\n",
    "                        all_tables.append({\n",
    "                            'page': page_num,\n",
    "                            'table_num': i+1,\n",
    "                            'dataframe': df_table,\n",
    "                            'html': html_table,\n",
    "                            'filename': csv_filename\n",
    "                        })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Structured extraction failed: {e}\")\n",
    "\n",
    "                print(\"-\" * 50)\n",
    "        else:\n",
    "            print(f\"No tables found on page {page_num}\")\n",
    "\n",
    "        page_num += 1\n",
    "\n",
    "    return all_tables\n",
    "\n",
    "# CHANGE THIS PATH TO YOUR BANK STATEMENT PDF\n",
    "pdf_path = \"/content/Revoult Bank statement.pdf\"  # <-- CHANGE THIS PATH\n",
    "\n",
    "# Extract tables\n",
    "extracted_tables = extract_tables_deepdoctection(pdf_path)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"Total tables extracted: {len(extracted_tables)}\")\n",
    "for table_info in extracted_tables:\n",
    "    print(f\"Page {table_info['page']}, Table {table_info['table_num']}: {table_info['filename']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdoctection as dd\n",
    "from IPython.core.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Instantiate the built-in analyzer\n",
    "analyzer = dd.get_dd_analyzer()\n",
    "\n",
    "# Analyze your bank statement PDF\n",
    "df = analyzer.analyze(path=\"/content/Barclays_uk_bank_statement.pdf\")\n",
    "df.reset_state()  # Initialize the pipeline\n",
    "\n",
    "# Process the document\n",
    "doc = iter(df)\n",
    "page = next(doc)\n",
    "\n",
    "# Visualize the detected layout and tables\n",
    "image = page.viz(show_figures=True, show_residual_layouts=True)\n",
    "plt.figure(figsize=(25, 17))\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "\n",
    "# Extract table data as HTML\n",
    "if page.tables:\n",
    "    HTML(page.tables[0].html)\n",
    "\n",
    "# Extract all text content\n",
    "print(page.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3999c8b",
   "metadata": {},
   "source": [
    "# for json or table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1cc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import pandas as pd\n",
    "\n",
    "# Read tables from PDF\n",
    "try:\n",
    "    tables = tabula.read_pdf('/content/Business Bank statment.pdf', pages='all')\n",
    "\n",
    "    # Check if any tables were found\n",
    "    if len(tables) > 0:\n",
    "        print(f\"Found {len(tables)} table(s)\")\n",
    "\n",
    "        # Process first table\n",
    "        df = tables[0]\n",
    "\n",
    "        # Export to JSON and Excel\n",
    "        df.to_json('bank_data.json', orient='records', indent=2)\n",
    "        df.to_excel('bank_data.xlsx', index=False)\n",
    "        print(\"Data exported successfully!\")\n",
    "    else:\n",
    "        print(\"No tables found in the PDF\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading PDF: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9d013",
   "metadata": {},
   "source": [
    "# trying deepdoctection with easy ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd504c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install easyocr pdf2image pandas openpyxl fuzzywuzzy python-levenshtein\n",
    "\n",
    "import easyocr\n",
    "import pdf2image\n",
    "import pandas as pd\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "\n",
    "class DynamicBankStatementExtractor:\n",
    "    def __init__(self):\n",
    "        self.reader = easyocr.Reader(['en'])\n",
    "\n",
    "        # Common column headers across different banks\n",
    "        self.common_headers = {\n",
    "            'date': ['date', 'transaction date', 'value date', 'posting date', 'trans date'],\n",
    "            'description': ['description', 'details', 'transaction details', 'reference', 'narrative', 'particulars'],\n",
    "            'debit': ['debit', 'withdrawal', 'amount dr', 'debits', 'outgoing'],\n",
    "            'credit': ['credit', 'deposit', 'amount cr', 'credits', 'incoming'],\n",
    "            'amount': ['amount', 'transaction amount', 'value'],\n",
    "            'balance': ['balance', 'running balance', 'available balance', 'closing balance'],\n",
    "            'reference': ['reference', 'ref', 'ref no', 'transaction id', 'cheque no'],\n",
    "            'category': ['category', 'type', 'transaction type']\n",
    "        }\n",
    "\n",
    "    def extract_bank_data_dynamic(self, pdf_path):\n",
    "        \"\"\"Extract bank data with dynamic column detection\"\"\"\n",
    "\n",
    "        # Convert PDF to images\n",
    "        images = pdf2image.convert_from_path(pdf_path)\n",
    "\n",
    "        all_text_data = []\n",
    "        detected_headers = []\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "            # Convert PIL Image to numpy array\n",
    "            image_np = np.array(image)\n",
    "\n",
    "            # Perform OCR\n",
    "            results = self.reader.readtext(image_np)\n",
    "\n",
    "            # Extract text with coordinates\n",
    "            text_with_coords = []\n",
    "            for (bbox, text, confidence) in results:\n",
    "                if confidence > 0.5:\n",
    "                    # Get center coordinates\n",
    "                    x_center = (bbox[0][0] + bbox[2][0]) / 2\n",
    "                    y_center = (bbox[0][1] + bbox[2][1]) / 2\n",
    "                    text_with_coords.append({\n",
    "                        'text': text.strip(),\n",
    "                        'x': x_center,\n",
    "                        'y': y_center,\n",
    "                        'confidence': confidence\n",
    "                    })\n",
    "\n",
    "            all_text_data.extend(text_with_coords)\n",
    "\n",
    "            # Detect headers from first page\n",
    "            if i == 0:\n",
    "                detected_headers = self.detect_column_headers(text_with_coords)\n",
    "\n",
    "        print(f\" Detected Headers: {detected_headers}\")\n",
    "\n",
    "        # Extract transactions\n",
    "        transactions = self.extract_transactions_dynamic(all_text_data, detected_headers)\n",
    "\n",
    "        # Create DataFrame\n",
    "        if transactions:\n",
    "            df = pd.DataFrame(transactions)\n",
    "\n",
    "            # Clean and standardize data\n",
    "            df = self.clean_dataframe(df)\n",
    "\n",
    "            # Export\n",
    "            df.to_excel('bank_transactions_dynamic.xlsx', index=False)\n",
    "            df.to_json('bank_transactions_dynamic.json', orient='records', indent=2)\n",
    "\n",
    "            return df\n",
    "        else:\n",
    "            print(\" No transactions found\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def detect_column_headers(self, text_data):\n",
    "        \"\"\"Dynamically detect column headers\"\"\"\n",
    "\n",
    "        potential_headers = []\n",
    "\n",
    "        # Look for text that might be headers\n",
    "        for item in text_data:\n",
    "            text = item['text'].lower()\n",
    "\n",
    "            # Check if text matches any common header patterns\n",
    "            for standard_name, variants in self.common_headers.items():\n",
    "                for variant in variants:\n",
    "                    similarity = fuzz.ratio(text, variant)\n",
    "                    if similarity > 70:  # 70% similarity threshold\n",
    "                        potential_headers.append({\n",
    "                            'original': item['text'],\n",
    "                            'standard': standard_name,\n",
    "                            'x': item['x'],\n",
    "                            'y': item['y'],\n",
    "                            'similarity': similarity\n",
    "                        })\n",
    "                        break\n",
    "\n",
    "        # Sort by Y coordinate (top to bottom) and X coordinate (left to right)\n",
    "        potential_headers.sort(key=lambda x: (x['y'], x['x']))\n",
    "\n",
    "        # Get unique headers (avoid duplicates)\n",
    "        unique_headers = []\n",
    "        used_standards = set()\n",
    "\n",
    "        for header in potential_headers:\n",
    "            if header['standard'] not in used_standards:\n",
    "                unique_headers.append(header)\n",
    "                used_standards.add(header['standard'])\n",
    "\n",
    "        # Sort by X coordinate for final column order\n",
    "        unique_headers.sort(key=lambda x: x['x'])\n",
    "\n",
    "        return unique_headers\n",
    "\n",
    "    def extract_transactions_dynamic(self, text_data, headers):\n",
    "        \"\"\"Extract transactions using dynamic headers\"\"\"\n",
    "\n",
    "        if not headers:\n",
    "            print(\"No headers detected, using fallback extraction\")\n",
    "            return self.fallback_extraction(text_data)\n",
    "\n",
    "        transactions = []\n",
    "\n",
    "        # Group text by rows (similar Y coordinates)\n",
    "        rows = self.group_text_by_rows(text_data)\n",
    "\n",
    "        for row in rows:\n",
    "            if self.is_likely_transaction_row(row, headers):\n",
    "                transaction = self.parse_transaction_row(row, headers)\n",
    "                if transaction:\n",
    "                    transactions.append(transaction)\n",
    "\n",
    "        return transactions\n",
    "\n",
    "    def group_text_by_rows(self, text_data, y_tolerance=20):\n",
    "        \"\"\"Group text items that are likely in the same row\"\"\"\n",
    "\n",
    "        # Sort by Y coordinate\n",
    "        sorted_data = sorted(text_data, key=lambda x: x['y'])\n",
    "\n",
    "        rows = []\n",
    "        current_row = []\n",
    "        current_y = None\n",
    "\n",
    "        for item in sorted_data:\n",
    "            if current_y is None or abs(item['y'] - current_y) <= y_tolerance:\n",
    "                current_row.append(item)\n",
    "                current_y = item['y'] if current_y is None else current_y\n",
    "            else:\n",
    "                if current_row:\n",
    "                    # Sort row items by X coordinate (left to right)\n",
    "                    current_row.sort(key=lambda x: x['x'])\n",
    "                    rows.append(current_row)\n",
    "                current_row = [item]\n",
    "                current_y = item['y']\n",
    "\n",
    "        # Add the last row\n",
    "        if current_row:\n",
    "            current_row.sort(key=lambda x: x['x'])\n",
    "            rows.append(current_row)\n",
    "\n",
    "        return rows\n",
    "\n",
    "    def is_likely_transaction_row(self, row, headers):\n",
    "        \"\"\"Determine if a row contains transaction data\"\"\"\n",
    "\n",
    "        row_text = ' '.join([item['text'] for item in row])\n",
    "\n",
    "        # Look for date patterns\n",
    "        date_patterns = [\n",
    "            r'\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4}',\n",
    "            r'\\d{2}[\\/\\-]\\d{2}[\\/\\-]\\d{4}',\n",
    "            r'\\d{1,2}\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)',\n",
    "        ]\n",
    "\n",
    "        has_date = any(re.search(pattern, row_text) for pattern in date_patterns)\n",
    "\n",
    "        # Look for amount patterns\n",
    "        amount_patterns = [\n",
    "            r'[\\£\\$\\€\\₹]?\\s*\\d+[,.]?\\d*\\.?\\d{2}',\n",
    "            r'\\d+[,.]?\\d*\\.?\\d{2}\\s*[\\£\\$\\€\\₹]?'\n",
    "        ]\n",
    "\n",
    "        has_amount = any(re.search(pattern, row_text) for pattern in amount_patterns)\n",
    "\n",
    "        return has_date or has_amount\n",
    "\n",
    "    def parse_transaction_row(self, row, headers):\n",
    "        \"\"\"Parse a transaction row based on detected headers\"\"\"\n",
    "\n",
    "        transaction = {}\n",
    "\n",
    "        # Initialize all detected columns\n",
    "        for header in headers:\n",
    "            transaction[header['standard']] = ''\n",
    "\n",
    "        # Map row items to columns based on X coordinates\n",
    "        header_x_coords = [(h['x'], h['standard']) for h in headers]\n",
    "        header_x_coords.sort()\n",
    "\n",
    "        for item in row:\n",
    "            # Find the closest header column\n",
    "            closest_header = None\n",
    "            min_distance = float('inf')\n",
    "\n",
    "            for header_x, header_name in header_x_coords:\n",
    "                distance = abs(item['x'] - header_x)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    closest_header = header_name\n",
    "\n",
    "            if closest_header and min_distance < 100:  # Within reasonable distance\n",
    "                # Append to existing value (in case multiple items map to same column)\n",
    "                if transaction[closest_header]:\n",
    "                    transaction[closest_header] += ' ' + item['text']\n",
    "                else:\n",
    "                    transaction[closest_header] = item['text']\n",
    "\n",
    "        # Clean up the transaction data\n",
    "        transaction = self.clean_transaction_data(transaction)\n",
    "\n",
    "        # Only return if we have essential data\n",
    "        if any(transaction.values()):\n",
    "            return transaction\n",
    "\n",
    "        return None\n",
    "\n",
    "    def fallback_extraction(self, text_data):\n",
    "        \"\"\"Fallback extraction when headers can't be detected\"\"\"\n",
    "\n",
    "        print(\"🔄 Using fallback extraction method\")\n",
    "\n",
    "        transactions = []\n",
    "        rows = self.group_text_by_rows(text_data)\n",
    "\n",
    "        for row in rows:\n",
    "            row_text = ' '.join([item['text'] for item in row])\n",
    "\n",
    "            # Use regex patterns to extract data\n",
    "            transaction = self.extract_with_patterns(row_text)\n",
    "            if transaction:\n",
    "                transactions.append(transaction)\n",
    "\n",
    "        return transactions\n",
    "\n",
    "    def extract_with_patterns(self, text):\n",
    "        \"\"\"Extract transaction data using regex patterns\"\"\"\n",
    "\n",
    "        transaction = {}\n",
    "\n",
    "        # Date extraction\n",
    "        date_match = re.search(r'\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4}', text)\n",
    "        if date_match:\n",
    "            transaction['date'] = date_match.group()\n",
    "\n",
    "        # Amount extraction\n",
    "        amount_matches = re.findall(r'[\\£\\$\\€\\₹]?\\s*\\d+[,.]?\\d*\\.?\\d{2}', text)\n",
    "        if amount_matches:\n",
    "            # First amount might be transaction amount, last might be balance\n",
    "            transaction['amount'] = amount_matches[0].strip()\n",
    "            if len(amount_matches) > 1:\n",
    "                transaction['balance'] = amount_matches[-1].strip()\n",
    "\n",
    "        # Description (remaining text after removing dates and amounts)\n",
    "        clean_text = re.sub(r'\\d{1,2}[\\/\\-]\\d{1,2}[\\/\\-]\\d{2,4}', '', clean_text) # Corrected: used clean_text instead of text\n",
    "        clean_text = re.sub(r'[\\£\\$\\€\\₹]?\\s*\\d+[,.]?\\d*\\.?\\d{2}', '', clean_text) # Corrected: used clean_text instead of text\n",
    "        transaction['description'] = clean_text.strip()\n",
    "\n",
    "        return transaction if any(transaction.values()) else None\n",
    "\n",
    "    def clean_transaction_data(self, transaction):\n",
    "        \"\"\"Clean and standardize transaction data\"\"\"\n",
    "\n",
    "        cleaned = {}\n",
    "\n",
    "        for key, value in transaction.items():\n",
    "            if value:\n",
    "                # Remove extra spaces and clean text\n",
    "                cleaned_value = ' '.join(value.split())\n",
    "\n",
    "                # Specific cleaning for different column types\n",
    "                if key == 'date':\n",
    "                    # Standardize date format\n",
    "                    cleaned_value = self.standardize_date(cleaned_value)\n",
    "                elif key in ['amount', 'debit', 'credit', 'balance']:\n",
    "                    # Clean amount format\n",
    "                    cleaned_value = self.clean_amount(cleaned_value)\n",
    "\n",
    "                cleaned[key] = cleaned_value\n",
    "            else:\n",
    "                cleaned[key] = ''\n",
    "\n",
    "        return cleaned\n",
    "\n",
    "    def standardize_date(self, date_str):\n",
    "        \"\"\"Standardize date format\"\"\"\n",
    "        # Try to parse and reformat date\n",
    "        import datetime\n",
    "\n",
    "        date_patterns = [\n",
    "            ('%d/%m/%Y', r'\\d{2}/\\d{2}/\\d{4}'),\n",
    "            ('%d-%m-%Y', r'\\d{2}-\\d{2}-\\d{4}'),\n",
    "            ('%d/%m/%y', r'\\d{2}/\\d{2}/\\d{2}'),\n",
    "            ('%d-%m-%y', r'\\d{2}-\\d{2}-\\d{2}'),\n",
    "        ]\n",
    "\n",
    "        for format_str, pattern in date_patterns:\n",
    "            if re.match(pattern, date_str):\n",
    "                try:\n",
    "                    date_obj = datetime.datetime.strptime(date_str, format_str)\n",
    "                    return date_obj.strftime('%Y-%m-%d')\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        return date_str  # Return original if can't parse\n",
    "\n",
    "    def clean_amount(self, amount_str):\n",
    "        \"\"\"Clean and standardize amount format\"\"\"\n",
    "        # Remove currency symbols and extra spaces\n",
    "        cleaned = re.sub(r'[^\\d.,\\-]', '', amount_str)\n",
    "        return cleaned\n",
    "\n",
    "    def clean_dataframe(self, df):\n",
    "        \"\"\"Final DataFrame cleaning\"\"\"\n",
    "\n",
    "        # Remove completely empty rows\n",
    "        df = df.dropna(how='all')\n",
    "\n",
    "        # Sort by date if date column exists\n",
    "        date_columns = [col for col in df.columns if 'date' in col.lower()]\n",
    "        if date_columns:\n",
    "            try:\n",
    "                df[date_columns[0]] = pd.to_datetime(df[date_columns[0]], errors='coerce')\n",
    "                df = df.sort_values(date_columns[0])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return df\n",
    "\n",
    "# Usage\n",
    "extractor = DynamicBankStatementExtractor()\n",
    "df = extractor.extract_bank_data_dynamic('/content/Revoult Bank statement.pdf')\n",
    "\n",
    "print(\"Extracted Data Preview:\")\n",
    "print(df.head())\n",
    "print(f\"\\notal Transactions: {len(df)}\")\n",
    "print(f\"Detected Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install poppler-utils"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
