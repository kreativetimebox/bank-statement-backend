{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8ba47d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in j:\\anaconda\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pytesseract in j:\\anaconda\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pillow in j:\\anaconda\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in j:\\anaconda\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in j:\\anaconda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate in j:\\anaconda\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: torch in j:\\anaconda\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in j:\\anaconda\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: packaging>=21.3 in j:\\anaconda\\lib\\site-packages (from pytesseract) (24.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in j:\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in j:\\anaconda\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in j:\\anaconda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in j:\\anaconda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in j:\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in j:\\anaconda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in j:\\anaconda\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in j:\\anaconda\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in j:\\anaconda\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in j:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in j:\\anaconda\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in j:\\anaconda\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in j:\\anaconda\\lib\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in j:\\anaconda\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in j:\\anaconda\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in j:\\anaconda\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in j:\\anaconda\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in j:\\anaconda\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in j:\\anaconda\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in j:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in j:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in j:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in j:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in j:\\anaconda\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in j:\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in j:\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in j:\\anaconda\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdf2image pytesseract pillow beautifulsoup4 pandas tabulate torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35c4f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutLMv3 model loaded successfully\n",
      "Processing: C:\\Users\\HP\\Desktop\\Bank Project\\Balraj\\Revoult Bank statement.pdf\n",
      "Using LayoutLMv3: Yes\n",
      "LayoutLMv3 processing error: LayoutLMv3ForTokenClassification.forward() got an unexpected keyword argument 'offset_mapping'\n",
      "+------------------------------------------------------------+-----------------------------------------------+-------------------------------------------+-----------------------------------------+-------------------------------------+-----------------+-----------+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| date                                                       | value date                                    | description                               | money out                               | money in                            | balance         | Extra_0   | Extra_1                      | Extra_2                                                                                                                                                                                                   |\n",
      "|------------------------------------------------------------+-----------------------------------------------+-------------------------------------------+-----------------------------------------+-------------------------------------+-----------------+-----------+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| 01 Jun                                                     | 20                                            |                                           | B/F                                     |                                     | 0.0             |           | =                            |                                                                                                                                                                                                           |\n",
      "|                                                            |                                               |                                           |                                         |                                     | 0.0             |           |                              | 3,844.28 Cr                                                                                                                                                                                               |\n",
      "| 01 Jun                                                     | 20 014                                        |                                           | VALDT/CHQ:01JUN20/DIRECT HISCOX         |                                     | 0.0             | 49.58     |                              | Jun20 DEB/ 3,794.70 Cr DD / 01223585                                                                                                                                                                      |\n",
      "|                                                            |                                               |                                           | ADVICE                                  | / 29346                             | 0.0             |           |                              | CONSULTANT                                                                                                                                                                                                |\n",
      "|                                                            |                                               |                                           | 0786743/0020332592                      |                                     | 0.0             |           |                              |                                                                                                                                                                                                           |\n",
      "| 30 Jun                                                     | 20 30 Jun                                     | 20                                        | VALDT/CHQ:30JUN20/ TRANSACTION          |                                     | 0.0             | 2.00      |                              | 3,792.70 Cr CHARGE                                                                                                                                                                                        |\n",
      "|                                                            |                                               |                                           | LFCEV_01-04-2020_30-06-2020             |                                     | 0.0             |           |                              |                                                                                                                                                                                                           |\n",
      "|                                                            | =                                             |                                           |                                         |                                     | 0.0             |           |                              | _——-                                                                                                                                                                                                      |\n",
      "| 30 Jun                                                     | 20 30 Jun                                     | 20                                        | VALDT/CHQ:30JUN20/ MAINTENANCE          |                                     | 0.0             | 00        | een                          | - ne ITI Cr — CHARGE                                                                                                                                                                                      |\n",
      "|                                                            |                                               |                                           | MBCEV_01-04-2020_30-06-2020             |                                     | 0.0             |           |                              |                                                                                                                                                                                                           |\n",
      "|                                                            |                                               |                                           | Grand Total                             |                                     | 0.0             | 66.58     |                              | 0.00                                                                                                                                                                                                      |\n",
      "| For FSCS https://www.bankofbarodauk.com/download-forms.htm | Information                                   | Sheet/exclusion                           |                                         | contact                             | 0.0             | visit or  |                              | list,please Bank of Baroda your                                                                                                                                                                           |\n",
      "| For further                                                | information                                   | about                                     | the compensation                        | refer                               | www.fscs.org.uk |           |                              | the FSCS to website at                                                                                                                                                                                    |\n",
      "| Message                                                    | for Customers:-                               |                                           | ;                                       |                                     | 0.0             |           |                              |                                                                                                                                                                                                           |\n",
      "| We have If there months Unrecognized recognise             | updated are no we may transactions it, please | the General transactions classify contact | Terms in the account your If any - your | (apart from inactive is immediately | 0.0             | believe,  | and charges. of is incorrect | & Conditions, which available are on account those generated by charges interest etc) for 24 and charges as apply may transactions showing statement, on your do not or you branch by telephone email. or |\n",
      "| Bank of                                                    | Baroda                                        | (UK) Ltdis                                | authorised                              |                                     | 0.0             |           |                              | andrequiated DD PRAR RGA                                                                                                                                                                                  |\n",
      "| Bank of                                                    | Baroda                                        | (UK) Ltd                                  | authorised is                           | reaulated                           | 0.0             |           |                              | and by PRA & ECA Dana 1 At4                                                                                                                                                                               |\n",
      "+------------------------------------------------------------+-----------------------------------------------+-------------------------------------------+-----------------------------------------+-------------------------------------+-----------------+-----------+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Total Transactions: 17\n",
      "Columns: date, value date, description, money out, money in, balance, Extra_0, Extra_1, Extra_2\n",
      "\n",
      "Amount Summary:\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import re\n",
    "# from pdf2image import convert_from_path\n",
    "# from PIL import Image\n",
    "# import pytesseract\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# from tabulate import tabulate\n",
    "# import shutil\n",
    "# import torch\n",
    "# from transformers import LayoutLMv3ForTokenClassification, LayoutLMv3Processor\n",
    "\n",
    "# # Global constants\n",
    "# USE_LAYOUTLMV3 = True\n",
    "# LAYOUTLMV3_MODEL_NAME = \"microsoft/layoutlmv3-base-finetuned-sroie\"\n",
    "# LINE_THRESHOLD = 15  # Pixels for line grouping\n",
    "# AMOUNT_KEYWORDS = ['amount', 'debit', 'credit', 'balance', 'total']\n",
    "# DATE_KEYWORDS = ['date', 'time']\n",
    "\n",
    "# # Load LayoutLMv3 model once\n",
    "# if USE_LAYOUTLMV3:\n",
    "#     try:\n",
    "#         layoutlmv3_processor = LayoutLMv3Processor.from_pretrained(LAYOUTLMV3_MODEL_NAME, apply_ocr=False)\n",
    "#         layoutlmv3_model = LayoutLMv3ForTokenClassification.from_pretrained(LAYOUTLMV3_MODEL_NAME)\n",
    "#         print(\"LayoutLMv3 model loaded successfully\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading LayoutLMv3 model: {e}\")\n",
    "#         USE_LAYOUTLMV3 = False\n",
    "\n",
    "# def convert_pdf_to_images(pdf_path, output_folder='temp_images'):\n",
    "#     \"\"\"Convert PDF pages to images.\"\"\"\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "#     images = convert_from_path(pdf_path)\n",
    "#     image_paths = []\n",
    "#     for i, image in enumerate(images):\n",
    "#         image_path = os.path.join(output_folder, f'page_{i+1}.jpg')\n",
    "#         image.save(image_path, 'JPEG')\n",
    "#         image_paths.append(image_path)\n",
    "#     return image_paths\n",
    "\n",
    "# def ocr_image(image_path):\n",
    "#     \"\"\"Perform OCR on an image and return hOCR output.\"\"\"\n",
    "#     hocr = pytesseract.image_to_pdf_or_hocr(image_path, extension='hocr', config='--psm 6')\n",
    "#     return hocr\n",
    "\n",
    "# def parse_hocr_to_words(hocr):\n",
    "#     \"\"\"Parse hOCR output to extract words and bounding boxes.\"\"\"\n",
    "#     soup = BeautifulSoup(hocr, 'html.parser')\n",
    "#     words = []\n",
    "#     for span in soup.find_all('span', class_='ocrx_word'):\n",
    "#         text = span.get_text().strip()\n",
    "#         title = span['title']\n",
    "#         bbox_match = re.search(r'bbox (\\d+) (\\d+) (\\d+) (\\d+)', title)\n",
    "#         if bbox_match:\n",
    "#             x_min, y_min, x_max, y_max = map(int, bbox_match.groups())\n",
    "#             words.append({'text': text, 'x_min': x_min, 'y_min': y_min, 'x_max': x_max, 'y_max': y_max})\n",
    "#     return words\n",
    "\n",
    "# def group_words_into_lines(words):\n",
    "#     \"\"\"Group words into lines based on vertical proximity.\"\"\"\n",
    "#     words.sort(key=lambda w: (w['y_min'], w['x_min']))\n",
    "#     lines = []\n",
    "#     current_line = []\n",
    "#     prev_y = None\n",
    "    \n",
    "#     for word in words:\n",
    "#         if prev_y is None or abs(word['y_min'] - prev_y) < LINE_THRESHOLD:\n",
    "#             current_line.append(word)\n",
    "#         else:\n",
    "#             lines.append(current_line)\n",
    "#             current_line = [word]\n",
    "#         prev_y = word['y_min']\n",
    "    \n",
    "#     if current_line:\n",
    "#         lines.append(current_line)\n",
    "    \n",
    "#     return lines\n",
    "\n",
    "# def extract_entities_with_layoutlmv3(image_path, words):\n",
    "#     \"\"\"Extract entities using LayoutLMv3 model.\"\"\"\n",
    "#     if not USE_LAYOUTLMV3:\n",
    "#         return words\n",
    "    \n",
    "#     try:\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "#         width, height = image.size\n",
    "        \n",
    "#         # Normalize bounding boxes to 0-1000 scale\n",
    "#         normalized_boxes = []\n",
    "#         for word in words:\n",
    "#             x0, y0, x1, y1 = word['x_min'], word['y_min'], word['x_max'], word['y_max']\n",
    "#             normalized_boxes.append([\n",
    "#                 int(1000 * x0 / width),\n",
    "#                 int(1000 * y0 / height),\n",
    "#                 int(1000 * x1 / width),\n",
    "#                 int(1000 * y1 / height),\n",
    "#             ])\n",
    "        \n",
    "#         tokens = [word['text'] for word in words]\n",
    "#         encoding = layoutlmv3_processor(\n",
    "#             image, \n",
    "#             tokens, \n",
    "#             boxes=normalized_boxes, \n",
    "#             return_offsets_mapping=True,\n",
    "#             return_tensors=\"pt\",\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\",\n",
    "#             max_length=512\n",
    "#         )\n",
    "        \n",
    "#         # Run model inference\n",
    "#         with torch.no_grad():\n",
    "#             outputs = layoutlmv3_model(**encoding)\n",
    "        \n",
    "#         # Process predictions\n",
    "#         predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "#         offset_mapping = encoding['offset_mapping'].squeeze().tolist()\n",
    "#         id2label = layoutlmv3_model.config.id2label\n",
    "        \n",
    "#         # Map predictions to words\n",
    "#         previous_word_idx = None\n",
    "#         for i, word_idx in enumerate(encoding.word_ids(0)):\n",
    "#             if word_idx is None or word_idx == previous_word_idx:\n",
    "#                 continue\n",
    "#             label = id2label[predictions[i]]\n",
    "#             words[word_idx]['label'] = label\n",
    "#             previous_word_idx = word_idx\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"LayoutLMv3 processing error: {e}\")\n",
    "    \n",
    "#     return words\n",
    "\n",
    "# def extract_table_from_lines(lines):\n",
    "#     \"\"\"Extract table structure from grouped lines.\"\"\"\n",
    "#     headers = []\n",
    "#     data_rows = []\n",
    "    \n",
    "#     # Identify header line using common keywords\n",
    "#     header_line = None\n",
    "#     for line in lines:\n",
    "#         line_text = ' '.join(w['text'].lower() for w in line)\n",
    "#         if any(kw in line_text for kw in DATE_KEYWORDS) and any(kw in line_text for kw in AMOUNT_KEYWORDS):\n",
    "#             header_line = line\n",
    "#             break\n",
    "    \n",
    "#     if not header_line:\n",
    "#         return [], []\n",
    "    \n",
    "#     # Extract headers\n",
    "#     headers = [word['text'] for word in header_line]\n",
    "    \n",
    "#     # Extract data rows\n",
    "#     for line in lines[lines.index(header_line) + 1:]:\n",
    "#         row = {}\n",
    "#         for i, word in enumerate(line):\n",
    "#             if i < len(headers):\n",
    "#                 header = headers[i]\n",
    "#                 row[header] = row.get(header, '') + ' ' + word['text']\n",
    "        \n",
    "#         # Clean row values\n",
    "#         for key in row:\n",
    "#             row[key] = row[key].strip()\n",
    "        \n",
    "#         if any(row.values()):  # Skip empty rows\n",
    "#             data_rows.append(row)\n",
    "    \n",
    "#     return headers, data_rows\n",
    "\n",
    "# def clean_extracted_data(data_rows, headers):\n",
    "#     \"\"\"Clean and normalize extracted data.\"\"\"\n",
    "#     cleaned_data = []\n",
    "    \n",
    "#     for row in data_rows:\n",
    "#         cleaned_row = {}\n",
    "#         for header in headers:\n",
    "#             value = row.get(header, '')\n",
    "            \n",
    "#             # Clean value based on header type\n",
    "#             if any(kw in header.lower() for kw in DATE_KEYWORDS):\n",
    "#                 # Standardize date formats\n",
    "#                 date_match = re.search(r'\\d{2}[-/]\\d{2}[-/]\\d{2,4}', value)\n",
    "#                 if date_match:\n",
    "#                     cleaned_row[header] = date_match.group()\n",
    "#                 else:\n",
    "#                     cleaned_row[header] = value\n",
    "            \n",
    "#             elif any(kw in header.lower() for kw in AMOUNT_KEYWORDS):\n",
    "#                 # Extract numerical values\n",
    "#                 amount_str = re.sub(r'[^\\d.-]', '', value)\n",
    "#                 try:\n",
    "#                     cleaned_row[header] = float(amount_str) if amount_str else 0.0\n",
    "#                 except ValueError:\n",
    "#                     cleaned_row[header] = value\n",
    "            \n",
    "#             else:\n",
    "#                 # Clean descriptive text\n",
    "#                 cleaned_value = re.sub(r'\\s+', ' ', value).strip()\n",
    "#                 cleaned_row[header] = cleaned_value\n",
    "        \n",
    "#         cleaned_data.append(cleaned_row)\n",
    "    \n",
    "#     return cleaned_data\n",
    "\n",
    "# def analyze_pdf(pdf_path):\n",
    "#     \"\"\"Analyze PDF and extract transaction data.\"\"\"\n",
    "#     image_paths = convert_pdf_to_images(pdf_path)\n",
    "#     all_data = []\n",
    "#     headers = []\n",
    "    \n",
    "#     for img_path in image_paths:\n",
    "#         # Get OCR results\n",
    "#         hocr = ocr_image(img_path)\n",
    "#         words = parse_hocr_to_words(hocr)\n",
    "        \n",
    "#         # Enhanced entity recognition\n",
    "#         words = extract_entities_with_layoutlmv3(img_path, words)\n",
    "        \n",
    "#         # Group words into lines\n",
    "#         lines = group_words_into_lines(words)\n",
    "        \n",
    "#         # Extract table data\n",
    "#         page_headers, page_data = extract_table_from_lines(lines)\n",
    "        \n",
    "#         if page_headers and not headers:\n",
    "#             headers = page_headers\n",
    "        \n",
    "#         if page_data:\n",
    "#             cleaned_page_data = clean_extracted_data(page_data, headers)\n",
    "#             all_data.extend(cleaned_page_data)\n",
    "        \n",
    "#         # Clean up image file\n",
    "#         os.remove(img_path)\n",
    "    \n",
    "#     # Clean up temporary folder\n",
    "#     if os.path.exists('temp_images'):\n",
    "#         shutil.rmtree('temp_images')\n",
    "    \n",
    "#     # Create DataFrame\n",
    "#     if headers and all_data:\n",
    "#         return pd.DataFrame(all_data, columns=headers)\n",
    "#     return None\n",
    "\n",
    "# def display_table(df):\n",
    "#     \"\"\"Display DataFrame in table format.\"\"\"\n",
    "#     if df is not None and not df.empty:\n",
    "#         print(tabulate(df, headers=df.columns, tablefmt='psql', showindex=False))\n",
    "#         print(f\"\\nTotal Transactions: {len(df)}\")\n",
    "#         print(f\"Columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "#         # Summary statistics\n",
    "#         amount_cols = [col for col in df.columns if any(kw in col.lower() for kw in AMOUNT_KEYWORDS)]\n",
    "#         if amount_cols:\n",
    "#             print(\"\\nAmount Summary:\")\n",
    "#             for col in amount_cols:\n",
    "#                 if pd.api.types.is_numeric_dtype(df[col]):\n",
    "#                     print(f\"- {col}:\")\n",
    "#                     print(f\"  Total: {df[col].sum():.2f}\")\n",
    "#                     print(f\"  Avg: {df[col].mean():.2f}\")\n",
    "#                     print(f\"  Min: {df[col].min():.2f}\")\n",
    "#                     print(f\"  Max: {df[col].max():.2f}\")\n",
    "#     else:\n",
    "#         print(\"No transaction data extracted.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Update with your PDF path\n",
    "#     pdf_path = r\"C:\\Users\\HP\\Desktop\\Bank Project\\Balraj\\Revoult Bank statement.pdf\"\n",
    "    \n",
    "#     if not os.path.isfile(pdf_path):\n",
    "#         print(f\"Error: File not found - {pdf_path}\")\n",
    "#     else:\n",
    "#         print(f\"Processing: {pdf_path}\")\n",
    "#         print(f\"Using LayoutLMv3: {'Yes' if USE_LAYOUTLMV3 else 'No'}\")\n",
    "        \n",
    "#         transaction_df = analyze_pdf(pdf_path)\n",
    "#         display_table(transaction_df)   \n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import shutil\n",
    "import torch\n",
    "from transformers import LayoutLMv3ForTokenClassification, LayoutLMv3Processor\n",
    "\n",
    "# Global constants\n",
    "USE_LAYOUTLMV3 = True\n",
    "LAYOUTLMV3_MODEL_NAME = \"microsoft/layoutlmv3-base\"\n",
    "LINE_THRESHOLD = 15  # Pixels for line grouping\n",
    "AMOUNT_KEYWORDS = ['amount', 'debit', 'credit', 'balance', 'total']\n",
    "DATE_KEYWORDS = ['date', 'time']\n",
    "\n",
    "# Load LayoutLMv3 model once\n",
    "if USE_LAYOUTLMV3:\n",
    "    try:\n",
    "        layoutlmv3_processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "        layoutlmv3_model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "        print(\"LayoutLMv3 model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LayoutLMv3 model: {e}\")\n",
    "        USE_LAYOUTLMV3 = False\n",
    "\n",
    "\n",
    "def convert_pdf_to_images(pdf_path, output_folder='temp_images'):\n",
    "    \"\"\"Convert PDF pages to images.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    images = convert_from_path(pdf_path)\n",
    "    image_paths = []\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(output_folder, f'page_{i+1}.jpg')\n",
    "        image.save(image_path, 'JPEG')\n",
    "        image_paths.append(image_path)\n",
    "    return image_paths\n",
    "\n",
    "def ocr_image(image_path):\n",
    "    \"\"\"Perform OCR on an image and return hOCR output.\"\"\"\n",
    "    hocr = pytesseract.image_to_pdf_or_hocr(image_path, extension='hocr', config='--psm 6')\n",
    "    return hocr\n",
    "\n",
    "def parse_hocr_to_words(hocr):\n",
    "    \"\"\"Parse hOCR output to extract words and bounding boxes.\"\"\"\n",
    "    soup = BeautifulSoup(hocr, 'html.parser')\n",
    "    words = []\n",
    "    for span in soup.find_all('span', class_='ocrx_word'):\n",
    "        text = span.get_text().strip()\n",
    "        title = span['title']\n",
    "        bbox_match = re.search(r'bbox (\\d+) (\\d+) (\\d+) (\\d+)', title)\n",
    "        if bbox_match:\n",
    "            x_min, y_min, x_max, y_max = map(int, bbox_match.groups())\n",
    "            words.append({'text': text, 'x_min': x_min, 'y_min': y_min, 'x_max': x_max, 'y_max': y_max})\n",
    "    return words\n",
    "\n",
    "def group_words_into_lines(words):\n",
    "    \"\"\"Group words into lines based on vertical proximity.\"\"\"\n",
    "    words.sort(key=lambda w: (w['y_min'], w['x_min']))\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    prev_y = None\n",
    "    \n",
    "    for word in words:\n",
    "        if prev_y is None or abs(word['y_min'] - prev_y) < LINE_THRESHOLD:\n",
    "            current_line.append(word)\n",
    "        else:\n",
    "            lines.append(current_line)\n",
    "            current_line = [word]\n",
    "        prev_y = word['y_min']\n",
    "    \n",
    "    if current_line:\n",
    "        lines.append(current_line)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def extract_entities_with_layoutlmv3(image_path, words):\n",
    "    \"\"\"Extract entities using LayoutLMv3 model.\"\"\"\n",
    "    if not USE_LAYOUTLMV3:\n",
    "        return words\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        width, height = image.size\n",
    "        \n",
    "        # Normalize bounding boxes to 0-1000 scale\n",
    "        normalized_boxes = []\n",
    "        for word in words:\n",
    "            x0, y0, x1, y1 = word['x_min'], word['y_min'], word['x_max'], word['y_max']\n",
    "            normalized_boxes.append([\n",
    "                int(1000 * x0 / width),\n",
    "                int(1000 * y0 / height),\n",
    "                int(1000 * x1 / width),\n",
    "                int(1000 * y1 / height),\n",
    "            ])\n",
    "        \n",
    "        tokens = [word['text'] for word in words]\n",
    "        encoding = layoutlmv3_processor(\n",
    "            image, \n",
    "            tokens, \n",
    "            boxes=normalized_boxes, \n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Run model inference\n",
    "        with torch.no_grad():\n",
    "            outputs = layoutlmv3_model(**encoding)\n",
    "        \n",
    "        # Process predictions\n",
    "        predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "        offset_mapping = encoding['offset_mapping'].squeeze().tolist()\n",
    "        id2label = layoutlmv3_model.config.id2label\n",
    "        \n",
    "        # Map predictions to words\n",
    "        previous_word_idx = None\n",
    "        for i, word_idx in enumerate(encoding.word_ids(0)):\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                continue\n",
    "            label = id2label[predictions[i]]\n",
    "            words[word_idx]['label'] = label\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"LayoutLMv3 processing error: {e}\")\n",
    "    \n",
    "    return words\n",
    "\n",
    "def extract_table_from_lines(lines):\n",
    "    \"\"\"Extract table structure from grouped lines using clean headers.\"\"\"\n",
    "    # Use EXPECTED_HEADERS and COLUMN_THRESHOLD from notebook variables\n",
    "    expected_headers = EXPECTED_HEADERS\n",
    "    column_threshold = COLUMN_THRESHOLD\n",
    "\n",
    "    headers = []\n",
    "    data_rows = []\n",
    "\n",
    "    header_line = None\n",
    "    for line in lines:\n",
    "        line_text = ' '.join(w['text'].lower() for w in line)\n",
    "        if all(kw in line_text for kw in ['date', 'description', 'balance']):\n",
    "            header_line = line\n",
    "            break\n",
    "\n",
    "    if not header_line:\n",
    "        return [], []\n",
    "\n",
    "    # Sort header words left to right and get X positions\n",
    "    header_line.sort(key=lambda w: w['x_min'])\n",
    "    column_positions = [w['x_min'] for w in header_line]\n",
    "\n",
    "    # Assign expected headers to these positions, trim or pad as needed\n",
    "    if len(column_positions) > len(expected_headers):\n",
    "        headers = expected_headers + [f\"Extra_{i}\" for i in range(len(column_positions) - len(expected_headers))]\n",
    "    else:\n",
    "        headers = expected_headers[:len(column_positions)]\n",
    "\n",
    "    # Now process data rows\n",
    "    for line in lines[lines.index(header_line) + 1:]:\n",
    "        row = [''] * len(headers)\n",
    "        for word in line:\n",
    "            # Find nearest column by x position within threshold\n",
    "            distances = [abs(word['x_min'] - col_x) for col_x in column_positions]\n",
    "            min_dist = min(distances)\n",
    "            col_idx = distances.index(min_dist)\n",
    "            # Only assign if within threshold, else skip\n",
    "            if min_dist <= column_threshold:\n",
    "                row[col_idx] += word['text'] + ' '\n",
    "            else:\n",
    "                # If no close column, skip or append to last column\n",
    "                row[-1] += word['text'] + ' '\n",
    "\n",
    "        # Clean whitespace\n",
    "        row = [val.strip() for val in row]\n",
    "        if any(row):\n",
    "            # Only zip up to the number of headers\n",
    "            data_rows.append(dict(zip(headers, row)))\n",
    "\n",
    "    return headers, data_rows \n",
    "\n",
    "def clean_extracted_data(data_rows, headers):\n",
    "    \"\"\"Clean and normalize extracted data.\"\"\"\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for row in data_rows:\n",
    "        cleaned_row = {}\n",
    "        for header in headers:\n",
    "            value = row.get(header, '')\n",
    "            \n",
    "            # Clean value based on header type\n",
    "            if any(kw in header.lower() for kw in DATE_KEYWORDS):\n",
    "                # Standardize date formats\n",
    "                date_match = re.search(r'\\d{2}[-/]\\d{2}[-/]\\d{2,4}', value)\n",
    "                if date_match:\n",
    "                    cleaned_row[header] = date_match.group()\n",
    "                else:\n",
    "                    cleaned_row[header] = value\n",
    "            \n",
    "            elif any(kw in header.lower() for kw in AMOUNT_KEYWORDS):\n",
    "                # Extract numerical values\n",
    "                amount_str = re.sub(r'[^\\d.-]', '', value)\n",
    "                try:\n",
    "                    cleaned_row[header] = float(amount_str) if amount_str else 0.0\n",
    "                except ValueError:\n",
    "                    cleaned_row[header] = value\n",
    "            \n",
    "            else:\n",
    "                # Clean descriptive text\n",
    "                cleaned_value = re.sub(r'\\s+', ' ', value).strip()\n",
    "                cleaned_row[header] = cleaned_value\n",
    "        \n",
    "        cleaned_data.append(cleaned_row)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def analyze_pdf(pdf_path):\n",
    "    \"\"\"Analyze PDF and extract transaction data.\"\"\"\n",
    "    image_paths = convert_pdf_to_images(pdf_path)\n",
    "    all_data = []\n",
    "    headers = []\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        # Get OCR results\n",
    "        hocr = ocr_image(img_path)\n",
    "        words = parse_hocr_to_words(hocr)\n",
    "        \n",
    "        # Enhanced entity recognition\n",
    "        words = extract_entities_with_layoutlmv3(img_path, words)\n",
    "        \n",
    "        # Group words into lines\n",
    "        lines = group_words_into_lines(words)\n",
    "        \n",
    "        # Extract table data\n",
    "        page_headers, page_data = extract_table_from_lines(lines)\n",
    "        \n",
    "        if page_headers and not headers:\n",
    "            headers = page_headers\n",
    "        \n",
    "        if page_data:\n",
    "            cleaned_page_data = clean_extracted_data(page_data, headers)\n",
    "            all_data.extend(cleaned_page_data)\n",
    "        \n",
    "        # Clean up image file\n",
    "        os.remove(img_path)\n",
    "    \n",
    "    # Clean up temporary folder\n",
    "    if os.path.exists('temp_images'):\n",
    "        shutil.rmtree('temp_images')\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if headers and all_data:\n",
    "        return pd.DataFrame(all_data, columns=headers)\n",
    "    return None\n",
    "\n",
    "def display_table(df):\n",
    "    \"\"\"Display DataFrame in table format.\"\"\"\n",
    "    if df is not None and not df.empty:\n",
    "        print(tabulate(df, headers=df.columns, tablefmt='psql', showindex=False))\n",
    "        print(f\"\\nTotal Transactions: {len(df)}\")\n",
    "        print(f\"Columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        amount_cols = [col for col in df.columns if any(kw in col.lower() for kw in AMOUNT_KEYWORDS)]\n",
    "        if amount_cols:\n",
    "            print(\"\\nAmount Summary:\")\n",
    "            for col in amount_cols:\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    print(f\"- {col}:\")\n",
    "                    print(f\"  Total: {df[col].sum():.2f}\")\n",
    "                    print(f\"  Avg: {df[col].mean():.2f}\")\n",
    "                    print(f\"  Min: {df[col].min():.2f}\")\n",
    "                    print(f\"  Max: {df[col].max():.2f}\")\n",
    "    else:\n",
    "        print(\"No transaction data extracted.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update with your PDF path\n",
    "    pdf_path = r\"C:\\Users\\HP\\Desktop\\Bank Project\\Balraj\\Revoult Bank statement.pdf\"\n",
    "    \n",
    "    if not os.path.isfile(pdf_path):\n",
    "        print(f\"Error: File not found - {pdf_path}\")\n",
    "    else:\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        print(f\"Using LayoutLMv3: {'Yes' if USE_LAYOUTLMV3 else 'No'}\")\n",
    "        \n",
    "        transaction_df = analyze_pdf(pdf_path)\n",
    "        display_table(transaction_df)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba0b4057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 17:40:35,368 - WARNING - Error loading LayoutLMv3 model: microsoft/layoutlmv3-base-finetoned-sroie is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`. Using fallback method.\n",
      "2025-08-07 17:40:35,381 - INFO - Created temporary image directory: C:\\Users\\HP\\AppData\\Local\\Temp\\tmpw_t2dxzg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Processing file: C:\\Users\\HP\\Desktop\\Bank Project\\Balraj\\Revoult Bank statement.pdf\n",
      "Using LayoutLMv3: No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 17:40:35,974 - INFO - Converted 1 pages to images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No transaction data found.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "import torch\n",
    "from transformers import LayoutLMv3ForTokenClassification, LayoutLMv3Processor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from pdf2image import convert_from_path\n",
    "import time  # Added for retry mechanism\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 🔧 Hardcode your PDF file path here\n",
    "PDF_PATH = r\"C:\\Users\\HP\\Desktop\\Bank Project\\Balraj\\Revoult Bank statement.pdf\"\n",
    "\n",
    "# Optional: Export to CSV — set to None to disable\n",
    "EXPORT_CSV_PATH = r\"C:\\Users\\HP\\Desktop\\Bank Project\\output.csv\"  # or None to skip exporting\n",
    "\n",
    "# LayoutLMv3 Configuration\n",
    "LAYOUTLMV3_MODEL_NAME = \"microsoft/layoutlmv3-base-finetoned-sroie\"\n",
    "AMOUNT_KEYWORDS = ['amount', 'debit', 'credit', 'balance', 'total', 'amt', 'value']\n",
    "DATE_KEYWORDS = ['date', 'time', 'transaction date', 'posted date']\n",
    "DESCRIPTION_KEYWORDS = ['description', 'merchant', 'details', 'narration', 'transaction', 'particulars']\n",
    "\n",
    "# Load LayoutLMv3 model\n",
    "try:\n",
    "    layoutlmv3_processor = LayoutLMv3Processor.from_pretrained(LAYOUTLMV3_MODEL_NAME, apply_ocr=False)\n",
    "    layoutlmv3_model = LayoutLMv3ForTokenClassification.from_pretrained(LAYOUTLMV3_MODEL_NAME)\n",
    "    logging.info(\"LayoutLMv3 model loaded successfully\")\n",
    "    USE_LAYOUTLMV3 = True\n",
    "except Exception as e:\n",
    "    logging.warning(f\"Error loading LayoutLMv3 model: {e}. Using fallback method.\")\n",
    "    USE_LAYOUTLMV3 = False\n",
    "\n",
    "def make_unique(headers):\n",
    "    seen = {}\n",
    "    unique = []\n",
    "    for i, col in enumerate(headers):\n",
    "        col = col.strip() if col else f\"Column_{i}\"\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            unique.append(f\"{col}_{seen[col]}\")\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            unique.append(col)\n",
    "    return unique\n",
    "\n",
    "def extract_entities_with_layoutlmv3(page_image, words):\n",
    "    \"\"\"Extract entities using LayoutLMv3 model.\"\"\"\n",
    "    if not USE_LAYOUTLMV3:\n",
    "        return words\n",
    "    \n",
    "    try:\n",
    "        width, height = page_image.size\n",
    "        \n",
    "        # Normalize bounding boxes to 0-1000 scale\n",
    "        normalized_boxes = []\n",
    "        for word in words:\n",
    "            x0, y0, x1, y1 = word['x_min'], word['y_min'], word['x_max'], word['y_max']\n",
    "            normalized_boxes.append([\n",
    "                int(1000 * x0 / width),\n",
    "                int(1000 * y0 / height),\n",
    "                int(1000 * x1 / width),\n",
    "                int(1000 * y1 / height),\n",
    "            ])\n",
    "        \n",
    "        tokens = [word['text'] for word in words]\n",
    "        encoding = layoutlmv3_processor(\n",
    "            page_image, \n",
    "            tokens, \n",
    "            boxes=normalized_boxes, \n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Run model inference\n",
    "        with torch.no_grad():\n",
    "            outputs = layoutlmv3_model(**encoding)\n",
    "        \n",
    "        # Process predictions\n",
    "        predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "        id2label = layoutlmv3_model.config.id2label\n",
    "        \n",
    "        # Map predictions to words\n",
    "        previous_word_idx = None\n",
    "        for i, word_idx in enumerate(encoding.word_ids(0)):\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                continue\n",
    "            label = id2label[predictions[i]]\n",
    "            words[word_idx]['label'] = label\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"LayoutLMv3 processing error: {e}\")\n",
    "    \n",
    "    return words\n",
    "\n",
    "def validate_and_correct_headers(headers, page_labeled_words):\n",
    "    \"\"\"Use LayoutLMv3 to validate and correct headers.\"\"\"\n",
    "    if not headers or not page_labeled_words or not USE_LAYOUTLMV3:\n",
    "        return headers\n",
    "    \n",
    "    # Identify header entities\n",
    "    header_entities = []\n",
    "    for word in page_labeled_words:\n",
    "        if word.get('label', '') in ['B-HEADER', 'I-HEADER']:\n",
    "            header_entities.append(word['text'])\n",
    "    \n",
    "    # If we found better header candidates, use them\n",
    "    if header_entities:\n",
    "        logging.info(f\"LayoutLMv3 identified header entities: {header_entities}\")\n",
    "        return header_entities\n",
    "    \n",
    "    return headers\n",
    "\n",
    "def validate_and_correct_row(row, headers, page_labeled_words):\n",
    "    \"\"\"Use LayoutLMv3 to validate and correct row data.\"\"\"\n",
    "    if not page_labeled_words or not USE_LAYOUTLMV3:\n",
    "        return row\n",
    "    \n",
    "    corrected_row = {}\n",
    "    for header in headers:\n",
    "        header_lower = header.lower()\n",
    "        cell_value = row.get(header, '')\n",
    "        \n",
    "        # Look for matching entities\n",
    "        entity_values = []\n",
    "        for word in page_labeled_words:\n",
    "            label = word.get('label', '')\n",
    "            text = word['text']\n",
    "            \n",
    "            if any(kw in header_lower for kw in DATE_KEYWORDS) and label in ['B-DATE', 'I-DATE']:\n",
    "                entity_values.append(text)\n",
    "            elif any(kw in header_lower for kw in AMOUNT_KEYWORDS) and label in ['B-AMOUNT', 'I-AMOUNT']:\n",
    "                entity_values.append(text)\n",
    "            elif label in ['B-COMPANY', 'I-COMPANY']:\n",
    "                entity_values.append(text)\n",
    "        \n",
    "        # Use entity values if found\n",
    "        if entity_values:\n",
    "            corrected_value = ' '.join(entity_values)\n",
    "            if corrected_value.strip():\n",
    "                corrected_row[header] = corrected_value\n",
    "                continue\n",
    "        \n",
    "        # Fallback to original value\n",
    "        corrected_row[header] = cell_value\n",
    "    \n",
    "    return corrected_row\n",
    "\n",
    "def clean_value(header, value):\n",
    "    \"\"\"Clean value based on header type.\"\"\"\n",
    "    header_lower = header.lower()\n",
    "    \n",
    "    if any(kw in header_lower for kw in DATE_KEYWORDS):\n",
    "        # Standardize date formats\n",
    "        date_match = re.search(r'\\d{2}[-/]\\d{2}[-/]\\d{2,4}', value)\n",
    "        if date_match:\n",
    "            return date_match.group()\n",
    "        return value\n",
    "    \n",
    "    elif any(kw in header_lower for kw in AMOUNT_KEYWORDS):\n",
    "        # Extract numerical values\n",
    "        amount_str = re.sub(r'[^\\d.-]', '', value)\n",
    "        try:\n",
    "            return float(amount_str) if amount_str else 0.0\n",
    "        except ValueError:\n",
    "            return value\n",
    "    \n",
    "    # Clean descriptive text\n",
    "    return re.sub(r'\\s+', ' ', value).strip()\n",
    "\n",
    "def safe_cleanup(temp_dir, max_retries=3, delay=1):\n",
    "    \"\"\"Safely clean up temporary directory with retries.\"\"\"\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            # Close any open file handles\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "            # Try to remove the directory\n",
    "            for root, dirs, files in os.walk(temp_dir, topdown=False):\n",
    "                for name in files:\n",
    "                    file_path = os.path.join(root, name)\n",
    "                    try:\n",
    "                        os.unlink(file_path)\n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Could not delete {file_path}: {e}\")\n",
    "                for name in dirs:\n",
    "                    os.rmdir(os.path.join(root, name))\n",
    "            os.rmdir(temp_dir)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Cleanup attempt {i+1} failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "    logging.error(f\"Failed to cleanup temporary directory after {max_retries} attempts\")\n",
    "    return False\n",
    "\n",
    "def parse_bank_statement(path):\n",
    "    all_dataframes = []\n",
    "    master_headers = None\n",
    "    page_image_paths = {}\n",
    "    temp_image_dir = tempfile.mkdtemp()\n",
    "    logging.info(f\"Created temporary image directory: {temp_image_dir}\")\n",
    "\n",
    "    try:\n",
    "        # Convert PDF to images and save to files\n",
    "        images = convert_from_path(path, output_folder=temp_image_dir, fmt='jpeg')\n",
    "        for i, image in enumerate(images):\n",
    "            image_path = os.path.join(temp_image_dir, f\"page_{i+1}.jpg\")\n",
    "            image.save(image_path, 'JPEG')\n",
    "            image.close()  # Close the image to release the file handle\n",
    "            page_image_paths[i+1] = image_path\n",
    "        logging.info(f\"Converted {len(images)} pages to images\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"PDF to image conversion failed: {e}\")\n",
    "        safe_cleanup(temp_image_dir)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            page_labeled_words = []\n",
    "            image_path = page_image_paths.get(page_num)\n",
    "            \n",
    "            if image_path and os.path.exists(image_path):\n",
    "                try:\n",
    "                    # Open image only when needed\n",
    "                    with Image.open(image_path) as page_image:\n",
    "                        # Extract words using pdfplumber\n",
    "                        words = page.extract_words(extra_attrs=[\"x0\", \"top\", \"x1\", \"bottom\"])\n",
    "                        page_words = []\n",
    "                        for word in words:\n",
    "                            page_words.append({\n",
    "                                'text': word['text'],\n",
    "                                'x_min': word['x0'],\n",
    "                                'y_min': word['top'],\n",
    "                                'x_max': word['x1'],\n",
    "                                'y_max': word['bottom']\n",
    "                            })\n",
    "                        \n",
    "                        # Process with LayoutLMv3\n",
    "                        if USE_LAYOUTLMV3:\n",
    "                            page_labeled_words = extract_entities_with_layoutlmv3(page_image, page_words)\n",
    "                        else:\n",
    "                            page_labeled_words = page_words\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing image for page {page_num}: {e}\")\n",
    "            \n",
    "            tables = page.extract_tables()\n",
    "            logging.info(f\"📄 Page {page_num}: Found {len(tables)} tables\")\n",
    "\n",
    "            for idx, table in enumerate(tables):\n",
    "                if not table or len(table) < 2:\n",
    "                    continue  # Skip empty or too-small tables\n",
    "\n",
    "                headers = table[0]\n",
    "                rows = table[1:]\n",
    "\n",
    "                # Validate and correct headers with LayoutLMv3\n",
    "                headers = validate_and_correct_headers(headers, page_labeled_words)\n",
    "                \n",
    "                # If headers are valid and not repeated junk rows, use them\n",
    "                cleaned_headers = [h.strip() if h else f\"Column_{i}\" for i, h in enumerate(headers)]\n",
    "                unique_headers = make_unique(cleaned_headers)\n",
    "\n",
    "                # Save headers from the first valid table\n",
    "                if master_headers is None and len(set(unique_headers)) == len(unique_headers):\n",
    "                    master_headers = unique_headers\n",
    "                elif master_headers and len(headers) == len(master_headers):\n",
    "                    # Assume headers are missing; treat entire table as data\n",
    "                    rows = table  # include all rows (no header)\n",
    "                    cleaned_headers = master_headers\n",
    "                else:\n",
    "                    continue  # Skip inconsistent tables\n",
    "\n",
    "                # Clean and validate rows\n",
    "                cleaned_rows = []\n",
    "                for row in rows:\n",
    "                    # Create dictionary for the row\n",
    "                    row_dict = {}\n",
    "                    for i, header in enumerate(cleaned_headers):\n",
    "                        if i < len(row):\n",
    "                            row_dict[header] = row[i].strip() if row[i] else ''\n",
    "                        else:\n",
    "                            row_dict[header] = ''\n",
    "                    \n",
    "                    # Validate and correct with LayoutLMv3\n",
    "                    row_dict = validate_and_correct_row(row_dict, cleaned_headers, page_labeled_words)\n",
    "                    \n",
    "                    # Clean values\n",
    "                    cleaned_row = {}\n",
    "                    for header, value in row_dict.items():\n",
    "                        cleaned_row[header] = clean_value(header, value)\n",
    "                    \n",
    "                    # Only include rows with at least one non-empty value\n",
    "                    if any(cleaned_row.values()):\n",
    "                        cleaned_rows.append(cleaned_row)\n",
    "\n",
    "                if not cleaned_rows:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    df = pd.DataFrame(cleaned_rows, columns=cleaned_headers)\n",
    "                    all_dataframes.append(df)\n",
    "                    logging.info(f\"✅ Page {page_num} Table {idx + 1}: Parsed {len(df)} rows\")\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"⚠️ Skipping malformed table on page {page_num}: {e}\")\n",
    "\n",
    "    # Clean up temporary files\n",
    "    safe_cleanup(temp_image_dir)\n",
    "\n",
    "    if not all_dataframes:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    return pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "def main():\n",
    "    if not os.path.isfile(PDF_PATH):\n",
    "        print(f\"❌ File does not exist: {PDF_PATH}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n📄 Processing file: {PDF_PATH}\")\n",
    "    print(f\"Using LayoutLMv3: {'Yes' if USE_LAYOUTLMV3 else 'No'}\")\n",
    "    \n",
    "    df = parse_bank_statement(PDF_PATH)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"⚠️ No transaction data found.\")\n",
    "    else:\n",
    "        print(\"\\n✅ Extracted Data:\\n\")\n",
    "        print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "        if EXPORT_CSV_PATH:\n",
    "            try:\n",
    "                df.to_csv(EXPORT_CSV_PATH, index=False)\n",
    "                print(f\"\\n📁 Exported to: {EXPORT_CSV_PATH}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to export CSV: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34fe157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Data saved to 'extracted_bank_statement_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import pdf2image\n",
    "\n",
    "# Function to convert PDF to images\n",
    "def pdf_to_images(pdf_path):\n",
    "    images = pdf2image.convert_from_path(pdf_path)\n",
    "    return images\n",
    "\n",
    "# Function to extract text from images using Tesseract\n",
    "def extract_text_from_image(image):\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "# Function to process images and extract data using LayoutLMv3\n",
    "def extract_data_with_layoutlm(images):\n",
    "    processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "    model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for image in images:\n",
    "        # Convert image to the required format\n",
    "        words = extract_text_from_image(image)\n",
    "        encoding = processor(image, words, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoding)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=2)\n",
    "        \n",
    "        # Collect data\n",
    "        for word, prediction in zip(words.split(), predictions[0].numpy()):\n",
    "            data.append((word, prediction))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to create a DataFrame from extracted data\n",
    "def create_dataframe(data):\n",
    "    df = pd.DataFrame(data, columns=[\"Word\", \"Prediction\"])\n",
    "    return df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\HP\\Desktop\\Bank Project\\Balraj\\Barclays_uk_bank_statement.pdf\"  # Path to your PDF file\n",
    "    images = pdf_to_images(pdf_path)\n",
    "    extracted_data = extract_data_with_layoutlm(images)\n",
    "    df = create_dataframe(extracted_data)\n",
    "    \n",
    "    # Save DataFrame to CSV\n",
    "    df.to_csv(\"extracted_bank_statement_data.csv\", index=False)\n",
    "    print(\"Data extraction complete. Data saved to 'extracted_bank_statement_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa23828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Error loading LayoutLMv3 model: microsoft/layoutlmv3-base-finetuned-sroie is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "Processing: C:\\Users\\HP\\Desktop\\Bank Project\\Balraj\\Revoult Bank statement.pdf\n",
      "Using LayoutLMv3: Yes\n",
      "+------------------------------------------------+----------------------------------+------------------------------------------------+----------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+----------------------+---------------------------------------------------+-----------+\n",
      "| Date                                           | Value                            | Date                                           | Description                                                                      | ==                                                                                                                                  | Money                                                               | Out                  | Moneyin.                                          |   Balance |\n",
      "|------------------------------------------------+----------------------------------+------------------------------------------------+----------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+----------------------+---------------------------------------------------+-----------|\n",
      "| B/F                                            | 20                               | B/F                                            |                                                                                  |                                                                                                                                     |                                                                     |                      | =                                                 |      0    |\n",
      "|                                                |                                  |                                                |                                                                                  |                                                                                                                                     |                                                                     |                      |                                                   |   3844.28 |\n",
      "|                                                | 014 Jun20                        |                                                | VALDT/CHQ:01JUN20/DIRECT HISCOX DD / 01223585                                    | DEB/                                                                                                                                |                                                                     | 49.58                |                                                   |   3794.7  |\n",
      "|                                                |                                  |                                                | ADVICE CONSULTANT                                                                | / 29346                                                                                                                             |                                                                     |                      |                                                   |      0    |\n",
      "|                                                |                                  |                                                | 0786743/0020332592                                                               |                                                                                                                                     |                                                                     |                      |                                                   |      0    |\n",
      "| 20                                             | 30 Jun                           | 20                                             | VALDT/CHQ:30JUN20/ TRANSACTION                                                   | CHARGE                                                                                                                              |                                                                     | 2.00                 |                                                   |   3792.7  |\n",
      "|                                                |                                  |                                                | LFCEV_01-04-2020_30-06-2020                                                      |                                                                                                                                     |                                                                     |                      |                                                   |      0    |\n",
      "|                                                | =                                |                                                |                                                                                  |                                                                                                                                     |                                                                     |                      |                                                   |      0    |\n",
      "| 20                                             | 30 Jun                           | 20                                             | VALDT/CHQ:30JUN20/ MAINTENANCE                                                   | CHARGE -                                                                                                                            |                                                                     | 00                   | een ne                                            |      0    |\n",
      "|                                                |                                  |                                                | MBCEV_01-04-2020_30-06-2020                                                      |                                                                                                                                     |                                                                     |                      |                                                   |      0    |\n",
      "|                                                |                                  |                                                | Grand Total                                                                      |                                                                                                                                     |                                                                     | 66.58                | 0.00                                              |      0    |\n",
      "|                                                | Information                      |                                                | https://www.bankofbarodauk.com/download-forms.htm Sheet/exclusion list,please    | contact your Bank of                                                                                                                | Baroda branch or                                                    | visit                |                                                   |      0    |\n",
      "| about                                          | information                      | about                                          | the compensation                                                                 | refer to the FSCS website                                                                                                           | at www.fscs.org.uk                                                  |                      |                                                   |      0    |\n",
      "| Customers:- ;                                  | for                              | Customers:- ;                                  |                                                                                  |                                                                                                                                     |                                                                     |                      |                                                   |      0    |\n",
      "| transactions transactions classify the contact | are we updated it, may no please | transactions transactions classify the contact | General your - in your If account the any Terms branch account transactions & as | Conditions, immediately inactive (apart is from and showing which by charges those telephone are on generated your available may or | statement, apply email. by on as us our per for you website example | schedule believe, of | charges is charges. incorrect and interest or you |     24    |\n",
      "| Ltdis                                          | Baroda (UK)                      | Ltdis                                          | authorised                                                                       | andrequiated DD PRAR                                                                                                                |                                                                     |                      | RGA                                               |      0    |\n",
      "| Ltd is                                         | Baroda (UK)                      | Ltd is                                         | authorised and                                                                   | reaulated by PRA & ECA                                                                                                              |                                                                     |                      |                                                   |     14    |\n",
      "+------------------------------------------------+----------------------------------+------------------------------------------------+----------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+----------------------+---------------------------------------------------+-----------+\n",
      "\n",
      "Total Transactions: 17\n",
      "Columns: Date, Value, Date, Description, ==, Money, Out, Moneyin., Balance\n",
      "\n",
      "Amount Summary:\n",
      "- Balance:\n",
      "  Total: 11469.68\n",
      "  Avg: 674.69\n",
      "  Min: 0.00\n",
      "  Max: 3844.28\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import shutil\n",
    "import torch\n",
    "from transformers import LayoutLMv3ForTokenClassification, LayoutLMv3Processor\n",
    "\n",
    "# Global constants\n",
    "USE_LAYOUTLMV3 = True\n",
    "LAYOUTLMV3_MODEL_NAME = \"microsoft/layoutlmv3-base-finetuned-sroie\"\n",
    "LINE_THRESHOLD = 15  # Pixels for line grouping\n",
    "AMOUNT_KEYWORDS = ['amount', 'debit', 'credit', 'balance', 'total']\n",
    "DATE_KEYWORDS = ['date', 'time']\n",
    "\n",
    "# Load LayoutLMv3 model once\n",
    "if USE_LAYOUTLMV3:\n",
    "    try:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        layoutlmv3_processor = LayoutLMv3Processor.from_pretrained(LAYOUTLMV3_MODEL_NAME, apply_ocr=False)\n",
    "        layoutlmv3_model = LayoutLMv3ForTokenClassification.from_pretrained(LAYOUTLMV3_MODEL_NAME).to(device)\n",
    "        print(\"LayoutLMv3 model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LayoutLMv3 model: {e}\")\n",
    "        USE_LAYOUTLMV3 = False\n",
    "\n",
    "def convert_pdf_to_images(pdf_path, output_folder='temp_images'):\n",
    "    \"\"\"Convert PDF pages to images.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        image_paths = []\n",
    "        for i, image in enumerate(images):\n",
    "            image_path = os.path.join(output_folder, f'page_{i+1}.jpg')\n",
    "            image.save(image_path, 'JPEG')\n",
    "            image_paths.append(image_path)\n",
    "        return image_paths\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting PDF to images: {e}\")\n",
    "        return []\n",
    "\n",
    "def ocr_image(image_path):\n",
    "    \"\"\"Perform OCR on an image and return hOCR output.\"\"\"\n",
    "    try:\n",
    "        hocr = pytesseract.image_to_pdf_or_hocr(image_path, extension='hocr', config='--psm 6')\n",
    "        return hocr\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed for {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_hocr_to_words(hocr):\n",
    "    \"\"\"Parse hOCR output to extract words and bounding boxes.\"\"\"\n",
    "    if not hocr:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(hocr, 'html.parser')\n",
    "        words = []\n",
    "        for span in soup.find_all('span', class_='ocrx_word'):\n",
    "            text = span.get_text().strip()\n",
    "            title = span.get('title', '')\n",
    "            bbox_match = re.search(r'bbox (\\d+) (\\d+) (\\d+) (\\d+)', title)\n",
    "            if bbox_match and text:\n",
    "                x_min, y_min, x_max, y_max = map(int, bbox_match.groups())\n",
    "                words.append({'text': text, 'x_min': x_min, 'y_min': y_min, 'x_max': x_max, 'y_max': y_max})\n",
    "        return words\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing hOCR: {e}\")\n",
    "        return []\n",
    "\n",
    "def group_words_into_lines(words):\n",
    "    \"\"\"Group words into lines based on vertical proximity.\"\"\"\n",
    "    if not words:\n",
    "        return []\n",
    "    \n",
    "    words.sort(key=lambda w: (w['y_min'], w['x_min']))\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    prev_y = None\n",
    "    \n",
    "    for word in words:\n",
    "        if prev_y is None or abs(word['y_min'] - prev_y) < LINE_THRESHOLD:\n",
    "            current_line.append(word)\n",
    "        else:\n",
    "            if current_line:\n",
    "                lines.append(current_line)\n",
    "            current_line = [word]\n",
    "        prev_y = word['y_min']\n",
    "    \n",
    "    if current_line:\n",
    "        lines.append(current_line)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def extract_entities_with_layoutlmv3(image_path, words):\n",
    "    \"\"\"Extract entities using LayoutLMv3 model.\"\"\"\n",
    "    if not USE_LAYOUTLMV3 or not words:\n",
    "        return words\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        width, height = image.size\n",
    "        \n",
    "        # Normalize bounding boxes to 0-1000 scale\n",
    "        normalized_boxes = []\n",
    "        for word in words:\n",
    "            x0, y0, x1, y1 = word['x_min'], word['y_min'], word['x_max'], word['y_max']\n",
    "            normalized_boxes.append([\n",
    "                max(0, int(1000 * x0 / width)),\n",
    "                max(0, int(1000 * y0 / height)),\n",
    "                min(1000, int(1000 * x1 / width)),\n",
    "                min(1000, int(1000 * y1 / height)),\n",
    "            ])\n",
    "        \n",
    "        tokens = [word['text'] for word in words]\n",
    "        encoding = layoutlmv3_processor(\n",
    "            image, \n",
    "            tokens, \n",
    "            boxes=normalized_boxes, \n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "        \n",
    "        # Run model inference\n",
    "        with torch.no_grad():\n",
    "            outputs = layoutlmv3_model(**encoding)\n",
    "        \n",
    "        # Process predictions\n",
    "        predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "        offset_mapping = encoding['offset_mapping'].squeeze().tolist()\n",
    "        id2label = layoutlmv3_model.config.id2label\n",
    "        \n",
    "        # Map predictions to words\n",
    "        previous_word_idx = None\n",
    "        for i, word_idx in enumerate(encoding.word_ids(0)):\n",
    "            if word_idx is None or word_idx == previous_word_idx:\n",
    "                continue\n",
    "            label = id2label[predictions[i]]\n",
    "            words[word_idx]['label'] = label\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        print(f\"LayoutLMv3 processed {len(words)} words\")\n",
    "    except Exception as e:\n",
    "        print(f\"LayoutLMv3 processing error: {e}\")\n",
    "    \n",
    "    return words\n",
    "\n",
    "def extract_table_from_lines(lines):\n",
    "    \"\"\"Extract table structure from grouped lines using grid-based approach.\"\"\"\n",
    "    if not lines:\n",
    "        return [], []\n",
    "    \n",
    "    # Find header line index using keywords\n",
    "    header_index = None\n",
    "    for idx, line in enumerate(lines):\n",
    "        line_text = ' '.join(w['text'].lower() for w in line)\n",
    "        if any(kw in line_text for kw in DATE_KEYWORDS) and any(kw in line_text for kw in AMOUNT_KEYWORDS):\n",
    "            header_index = idx\n",
    "            break\n",
    "    \n",
    "    # Fallback: find first line with multiple columns\n",
    "    if header_index is None:\n",
    "        for idx, line in enumerate(lines):\n",
    "            if len(line) >= 3:  # At least 3 columns\n",
    "                header_index = idx\n",
    "                break\n",
    "    \n",
    "    if header_index is None:\n",
    "        return [], []\n",
    "    \n",
    "    header_line = lines[header_index]\n",
    "    # Sort header words by x-position\n",
    "    header_line_sorted = sorted(header_line, key=lambda w: w['x_min'])\n",
    "    headers = [word['text'] for word in header_line_sorted]\n",
    "    \n",
    "    # Calculate column boundaries using header positions\n",
    "    boundaries = []\n",
    "    for i in range(len(header_line_sorted) - 1):\n",
    "        boundary = (header_line_sorted[i]['x_max'] + header_line_sorted[i+1]['x_min']) / 2\n",
    "        boundaries.append(boundary)\n",
    "    \n",
    "    # Add left and right boundaries\n",
    "    boundaries = [0] + boundaries + [100000]  # Large number for right boundary\n",
    "    \n",
    "    data_rows = []\n",
    "    # Process subsequent lines\n",
    "    for line in lines[header_index+1:]:\n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Sort words by x-position\n",
    "        sorted_line = sorted(line, key=lambda w: w['x_min'])\n",
    "        row = [''] * len(headers)\n",
    "        \n",
    "        for word in sorted_line:\n",
    "            center = (word['x_min'] + word['x_max']) / 2\n",
    "            # Find matching column\n",
    "            for col_idx in range(len(boundaries)-1):\n",
    "                if boundaries[col_idx] <= center < boundaries[col_idx+1]:\n",
    "                    # Append text to column\n",
    "                    if row[col_idx]:\n",
    "                        row[col_idx] += ' ' + word['text']\n",
    "                    else:\n",
    "                        row[col_idx] = word['text']\n",
    "                    break\n",
    "        \n",
    "        # Convert to dictionary\n",
    "        row_dict = {header: value.strip() for header, value in zip(headers, row)}\n",
    "        data_rows.append(row_dict)\n",
    "    \n",
    "    return headers, data_rows\n",
    "\n",
    "def clean_extracted_data(data_rows, headers):\n",
    "    \"\"\"Clean and normalize extracted data.\"\"\"\n",
    "    cleaned_data = []\n",
    "    \n",
    "    for row in data_rows:\n",
    "        cleaned_row = {}\n",
    "        for header in headers:\n",
    "            value = row.get(header, '')\n",
    "            \n",
    "            # Clean value based on header type\n",
    "            if any(kw in header.lower() for kw in DATE_KEYWORDS):\n",
    "                # Standardize date formats\n",
    "                date_match = re.search(r'\\d{2}[-/]\\d{2}[-/]\\d{2,4}', value)\n",
    "                if date_match:\n",
    "                    cleaned_row[header] = date_match.group()\n",
    "                else:\n",
    "                    cleaned_row[header] = value\n",
    "            \n",
    "            elif any(kw in header.lower() for kw in AMOUNT_KEYWORDS):\n",
    "                # Extract numerical values with currency symbols\n",
    "                amount_str = re.sub(r'[^\\d.,-]', '', value)\n",
    "                # Handle thousand separators\n",
    "                amount_str = amount_str.replace(',', '').replace(' ', '')\n",
    "                try:\n",
    "                    cleaned_row[header] = float(amount_str) if amount_str else 0.0\n",
    "                except ValueError:\n",
    "                    cleaned_row[header] = value\n",
    "            \n",
    "            else:\n",
    "                # Clean descriptive text\n",
    "                cleaned_value = re.sub(r'\\s+', ' ', value).strip()\n",
    "                cleaned_row[header] = cleaned_value\n",
    "        \n",
    "        cleaned_data.append(cleaned_row)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def analyze_pdf(pdf_path):\n",
    "    \"\"\"Analyze PDF and extract transaction data.\"\"\"\n",
    "    try:\n",
    "        image_paths = convert_pdf_to_images(pdf_path)\n",
    "        if not image_paths:\n",
    "            print(\"No images generated from PDF\")\n",
    "            return None\n",
    "            \n",
    "        all_data = []\n",
    "        headers = []\n",
    "        \n",
    "        for img_path in image_paths:\n",
    "            # Get OCR results\n",
    "            hocr = ocr_image(img_path)\n",
    "            if not hocr:\n",
    "                continue\n",
    "                \n",
    "            words = parse_hocr_to_words(hocr)\n",
    "            if not words:\n",
    "                print(f\"No words extracted from {img_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Enhanced entity recognition\n",
    "            words = extract_entities_with_layoutlmv3(img_path, words)\n",
    "            \n",
    "            # Group words into lines\n",
    "            lines = group_words_into_lines(words)\n",
    "            if not lines:\n",
    "                print(f\"No lines formed from {img_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Extract table data\n",
    "            page_headers, page_data = extract_table_from_lines(lines)\n",
    "            if not page_headers or not page_data:\n",
    "                print(f\"No table data extracted from {img_path}\")\n",
    "                continue\n",
    "                \n",
    "            if not headers:\n",
    "                headers = page_headers\n",
    "                \n",
    "            cleaned_page_data = clean_extracted_data(page_data, headers)\n",
    "            all_data.extend(cleaned_page_data)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        if headers and all_data:\n",
    "            return pd.DataFrame(all_data, columns=headers)\n",
    "        return None\n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        if os.path.exists('temp_images'):\n",
    "            shutil.rmtree('temp_images')\n",
    "\n",
    "def display_table(df):\n",
    "    \"\"\"Display DataFrame in table format.\"\"\"\n",
    "    if df is not None and not df.empty:\n",
    "        print(tabulate(df, headers=df.columns, tablefmt='psql', showindex=False))\n",
    "        print(f\"\\nTotal Transactions: {len(df)}\")\n",
    "        print(f\"Columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "        # Summary statistics\n",
    "        amount_cols = [col for col in df.columns if any(kw in col.lower() for kw in AMOUNT_KEYWORDS)]\n",
    "        if amount_cols:\n",
    "            print(\"\\nAmount Summary:\")\n",
    "            for col in amount_cols:\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    print(f\"- {col}:\")\n",
    "                    print(f\"  Total: {df[col].sum():.2f}\")\n",
    "                    print(f\"  Avg: {df[col].mean():.2f}\")\n",
    "                    print(f\"  Min: {df[col].min():.2f}\")\n",
    "                    print(f\"  Max: {df[col].max():.2f}\")\n",
    "    else:\n",
    "        print(\"No transaction data extracted.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update with your PDF path\n",
    "    pdf_path = r\"C:\\Users\\HP\\Desktop\\Bank Project\\Balraj\\Revoult Bank statement.pdf\"\n",
    "    \n",
    "    if not os.path.isfile(pdf_path):\n",
    "        print(f\"Error: File not found - {pdf_path}\")\n",
    "    else:\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        print(f\"Using LayoutLMv3: {'No' if USE_LAYOUTLMV3 else 'Yes'}\")\n",
    "        \n",
    "        transaction_df = analyze_pdf(pdf_path)\n",
    "        display_table(transaction_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
